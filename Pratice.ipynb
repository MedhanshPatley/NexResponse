{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-08T02:58:17.895528Z",
     "start_time": "2024-10-08T02:57:57.137988900Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy \n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import OPTForCausalLM, GPT2Tokenizer\n",
    "import requests\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer,util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T02:58:18.900067600Z",
     "start_time": "2024-10-08T02:58:17.901530700Z"
    }
   },
   "id": "869192d13569671e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def embedding(qs):\n",
    "    em =model.encode(qs)\n",
    "    return em"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T02:58:18.950902800Z",
     "start_time": "2024-10-08T02:58:18.903070700Z"
    }
   },
   "id": "b6b9765a345761db"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def similarity(query1, query2):\n",
    "    sim= util.cos_sim(query1, query2)\n",
    "    return sim"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T02:58:18.982977500Z",
     "start_time": "2024-10-08T02:58:18.918363500Z"
    }
   },
   "id": "e2888c5e9b785a1e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def save_to_txt(embedding, filename):\n",
    "    with open(filename,'w') as f:\n",
    "        for value in embedding:\n",
    "            f.write(str(value)+ ',')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T02:58:18.983728Z",
     "start_time": "2024-10-08T02:58:18.927093500Z"
    }
   },
   "id": "d65a566e970084f6"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def prepare_query(corpus,se,qs):\n",
    "    em = embedding(qs)\n",
    "    q_sim_score= similarity(se,em)\n",
    "    top_5=sorted(q_sim_score,reverse=True)[0:5]\n",
    "    data = {}\n",
    "    for sentence, score in zip(corpus,top_5):\n",
    "        # data.append([sentence, score])\n",
    "         data[sentence]=score\n",
    "    # df = pd.DataFrame(data, columns=['Sentence', 'Similarity Score'])\n",
    "    \n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T02:59:18.607655600Z",
     "start_time": "2024-10-08T02:59:18.550166600Z"
    }
   },
   "id": "37e8aa6c7a4810eb"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def prepare_prompt(top_sent,qs):\n",
    "    top_sentences =list(x.keys())\n",
    "    prompt =  qs +\"\\n\".join(top_sentences) + \"\\n\"\n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T03:05:09.782880900Z",
     "start_time": "2024-10-08T03:05:09.760630200Z"
    }
   },
   "id": "8b5dd67499741c73"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "context=[\" Generally speaking, image content may include both visual and semantic content.\",\n",
    "\"Visual content can be very general or domain specific. General visual content include color, texture, shape, spatial relationship, etc.\",\n",
    "\" Domain specific visual content, like human faces, is application dependent and may involve domain knowledge.\",\n",
    "\"Semantic content is obtained either by textual annotation or by complex inference procedures based on visual content.\", \n",
    "\"This chapter concentrates on general visual contents descriptions.\",\n",
    "\"Later chapters discuss domain specific and semantic contents.\",\n",
    "\"A good visual content descriptor should be invariant to the accidental variance introduced by the imaging process (e.g., the variation of the illuminant of the scene).However, there is a tradeoff between the invariance and the discriminative power of visual features, since a very wide class of invariance loses the ability to discriminate between essential differences.\",\n",
    "\"Invariant description has been largely investigated in computer vision (like object recognition), but is relatively new in image retrieval [8].\",\n",
    "\"A visual content descriptor can be either global or local.\",\n",
    "\"A global descriptor uses the visual features of the whole image, whereas a local descriptor uses the visual features of regions or objects to describe the image content.\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T03:05:10.413563700Z",
     "start_time": "2024-10-08T03:05:10.399310700Z"
    }
   },
   "id": "71114dcf3f5af4f"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "[' Generally speaking, image content may include both visual and semantic content.',\n 'Visual content can be very general or domain specific. General visual content include color, texture, shape, spatial relationship, etc.',\n ' Domain specific visual content, like human faces, is application dependent and may involve domain knowledge.',\n 'Semantic content is obtained either by textual annotation or by complex inference procedures based on visual content.',\n 'This chapter concentrates on general visual contents descriptions.',\n 'Later chapters discuss domain specific and semantic contents.',\n 'A good visual content descriptor should be invariant to the accidental variance introduced by the imaging process (e.g., the variation of the illuminant of the scene).However, there is a tradeoff between the invariance and the discriminative power of visual features, since a very wide class of invariance loses the ability to discriminate between essential differences.',\n 'Invariant description has been largely investigated in computer vision (like object recognition), but is relatively new in image retrieval [8].',\n 'A visual content descriptor can be either global or local.',\n 'A global descriptor uses the visual features of the whole image, whereas a local descriptor uses the visual features of regions or objects to describe the image content.']"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T03:05:11.196845600Z",
     "start_time": "2024-10-08T03:05:11.179022700Z"
    }
   },
   "id": "a6b70f3a89ff185b"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "(10, 384)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = embedding(context)\n",
    "s1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T03:05:12.028307700Z",
     "start_time": "2024-10-08T03:05:11.723076200Z"
    }
   },
   "id": "998ee81baaec82dd"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.1276735 , -0.03150531,  0.02126714, ...,  0.01614668,\n         0.07686063, -0.07784723],\n       [ 0.09080701, -0.06885422,  0.0145974 , ...,  0.05979102,\n         0.03004542, -0.03163181],\n       [ 0.01572219, -0.06374924,  0.00195766, ...,  0.04744347,\n         0.04895998,  0.02553334],\n       ...,\n       [-0.01173849,  0.06075974,  0.02245931, ...,  0.07059798,\n        -0.00260606, -0.09241313],\n       [ 0.02216985, -0.02991777, -0.00841484, ...,  0.0925092 ,\n         0.02544158, -0.0117879 ],\n       [ 0.01890386,  0.0006501 ,  0.05565466, ...,  0.07672178,\n        -0.02809634, -0.02797282]], dtype=float32)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T03:05:13.347415900Z",
     "start_time": "2024-10-08T03:05:13.319609400Z"
    }
   },
   "id": "e7514199fca79a2b"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "save_to_txt(s1,'embedding.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-06T12:28:05.402952200Z",
     "start_time": "2024-10-06T12:28:05.285785400Z"
    }
   },
   "id": "67526dc41f4dd317"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "queries = [\"What are the components of general visual content in an image?\",]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T03:05:17.611734200Z",
     "start_time": "2024-10-08T03:05:17.576144600Z"
    }
   },
   "id": "7c9960f4745e22b6"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "['What are the components of general visual content in an image?']"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T03:05:18.274191400Z",
     "start_time": "2024-10-08T03:05:18.206856100Z"
    }
   },
   "id": "98004c7c0d989c6d"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "x= prepare_query(context,s1,queries)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T03:05:19.557423100Z",
     "start_time": "2024-10-08T03:05:19.516939100Z"
    }
   },
   "id": "1669004308fa38b1"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "{' Generally speaking, image content may include both visual and semantic content.': tensor([0.7373]),\n 'Visual content can be very general or domain specific. General visual content include color, texture, shape, spatial relationship, etc.': tensor([0.6963]),\n ' Domain specific visual content, like human faces, is application dependent and may involve domain knowledge.': tensor([0.5377]),\n 'Semantic content is obtained either by textual annotation or by complex inference procedures based on visual content.': tensor([0.5344]),\n 'This chapter concentrates on general visual contents descriptions.': tensor([0.4998])}"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T03:05:20.290487300Z",
     "start_time": "2024-10-08T03:05:20.267222300Z"
    }
   },
   "id": "327b3a8e202ebc81"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "[' Generally speaking, image content may include both visual and semantic content.',\n 'Visual content can be very general or domain specific. General visual content include color, texture, shape, spatial relationship, etc.',\n ' Domain specific visual content, like human faces, is application dependent and may involve domain knowledge.',\n 'Semantic content is obtained either by textual annotation or by complex inference procedures based on visual content.',\n 'This chapter concentrates on general visual contents descriptions.']"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T03:05:21.566225Z",
     "start_time": "2024-10-08T03:05:21.518544300Z"
    }
   },
   "id": "f4defb88329e2f66"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "query = \"What are the components of general visual content in an image?\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T03:05:22.441235Z",
     "start_time": "2024-10-08T03:05:22.417910400Z"
    }
   },
   "id": "dc4c9a7ec001dd9c"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "prompt  =prepare_prompt(x,query)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T03:05:22.988927200Z",
     "start_time": "2024-10-08T03:05:22.969221300Z"
    }
   },
   "id": "36e5fece5915cc3d"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "'What are the components of general visual content in an image? Generally speaking, image content may include both visual and semantic content.\\nVisual content can be very general or domain specific. General visual content include color, texture, shape, spatial relationship, etc.\\n Domain specific visual content, like human faces, is application dependent and may involve domain knowledge.\\nSemantic content is obtained either by textual annotation or by complex inference procedures based on visual content.\\nThis chapter concentrates on general visual contents descriptions.\\n'"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T03:05:23.708847100Z",
     "start_time": "2024-10-08T03:05:23.683580300Z"
    }
   },
   "id": "24d0db2a04fed6ae"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\Medhansh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\transformers\\generation\\utils.py:1268: UserWarning: Input length of input_ids is 102, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    max_length=100,\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T03:33:12.301379200Z",
     "start_time": "2024-10-08T03:32:43.972993600Z"
    }
   },
   "id": "e8e6dd5d66588ae9"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the components of general visual content in an image? Generally speaking, image content may include both visual and semantic content.\n",
      "Visual content can be very general or domain specific. General visual content include color, texture, shape, spatial relationship, etc.\n",
      " Domain specific visual content, like human faces, is application dependent and may involve domain knowledge.\n",
      "Semantic content is obtained either by textual annotation or by complex inference procedures based on visual content.\n",
      "This chapter concentrates on general visual contents descriptions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gen_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T03:33:12.380414100Z",
     "start_time": "2024-10-08T03:33:12.335133100Z"
    }
   },
   "id": "19abf38d5cd6fdda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "691a31c550b9f861"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tensorflow",
   "language": "python",
   "display_name": "Python 3.9 (tensorflow)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
